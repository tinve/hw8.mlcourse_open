{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook\n",
    "from subprocess import call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holdout_cv_vw(train_file_vw,\n",
    "                  train_size=10000,\n",
    "                  train_params='',\n",
    "                  to_dir = 'cv',\n",
    "                  scoring=mean_absolute_error,\n",
    "                  make_sets = True,\n",
    "                  mute = False):\n",
    "    \n",
    "    \"\"\" This function performs holdout cross validation with wovpalwabbit.\n",
    "        It takes a path to training data in vw format as an argument and\n",
    "        creates train and validation sets, as well a as model and prediction files\n",
    "        in subdirectory to_dir. In the end, the model is retrained on both\n",
    "        train and test data and saved as file. Temporaryfiles are removed.   \n",
    "\n",
    "    Args:\n",
    "        train_file_vw: path to training data in vw format\n",
    "        train_size:    size of train set, the rest is use as validation set\n",
    "        train_params:  string of options for vowpal training \n",
    "        to_dir:        name of the folder to save models to\n",
    "        scoring:       pass function that returns score list of predictions and list of targets\n",
    "        make_sets:     if True, create train and validation sets; else use existing ones\n",
    "        mute:          print bash commands or not\n",
    "\n",
    "    Returns:\n",
    "        a score on holdout set\n",
    "    \"\"\"\n",
    "    \n",
    "    path = '/'.join(train_file_vw.split('/')[:-1]) + '/' + to_dir + '/'\n",
    "    \n",
    "    if make_sets:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        # need codecs.open for texts with Cyrillic characters\n",
    "        # without them, use simple open and drop 'utf-8'\n",
    "        y = []\n",
    "        with codecs.open(train_file_vw, 'r', 'utf-8') as data, \\\n",
    "             codecs.open(path+'train.vw', 'w', 'utf-8') as train, \\\n",
    "             codecs.open(path+'valid.vw', 'w', 'utf-8') as valid, \\\n",
    "            open(path+'train_y.txt', 'w') as train_y, \\\n",
    "            open(path+'valid_y.txt', 'w') as valid_y:\n",
    "            \n",
    "            for i, line in enumerate(data):\n",
    "                if i < train_size:\n",
    "                    train.write(line)\n",
    "                    train_y.write(line.split(' ', 1)[0]+'\\n')\n",
    "                else:\n",
    "                    valid.write(line)\n",
    "                    valid_y.write(line.split(' ', 1)[0]+'\\n')\n",
    "\n",
    "    \n",
    "    # train a model on validation training set\n",
    "    command = 'vw -c -k -d ' + path+'train.vw -f ' + path + 'model.vw ' + train_params\n",
    "    if not mute: print command\n",
    "    command = command.split()\n",
    "    call(command)\n",
    "\n",
    "    # make a prediction on train set\n",
    "    command = 'vw -i ' + path+'model.vw -t -d ' + path + 'train.vw -p ' + path + 'train_p.txt'\n",
    "    if not mute: print command\n",
    "    command = command.split()\n",
    "    call(command)\n",
    "  \n",
    "    # make a prediction on holdout set\n",
    "    command = 'vw -i ' + path+'model.vw -t -d ' +path+'valid.vw -p ' + path+'valid_p.txt'\n",
    "    if not mute: print command\n",
    "    command = command.split()\n",
    "    call(command)\n",
    "\n",
    "    with open(path+'train_p.txt') as f:\n",
    "        train_p = [float(label) for label in f.readlines()]\n",
    "    os.remove(path+'train_p.txt')\n",
    "    with open(path+'train_y.txt') as f:\n",
    "        train_y = [float(label) for label in f.readlines()]\n",
    "    train_score = scoring(train_p, train_y)\n",
    "    \n",
    "    with open(path+'valid_p.txt') as f:\n",
    "        valid_p = [float(label) for label in f.readlines()]\n",
    "    os.remove(path+'valid_p.txt')\n",
    "    with open(path+'valid_y.txt') as f:\n",
    "        valid_y = [float(label) for label in f.readlines()]\n",
    "    valid_score = scoring(valid_p, valid_y)\n",
    "    \n",
    "    # train a model on full training set\n",
    "#     command = 'vw -c -k -d ' + train_file_vw + ' -f ' + path+'model.vw ' + train_params\n",
    "#     if not mute: print command\n",
    "#     command = command.split()\n",
    "#     call(command)    \n",
    "#     model = path+'model.vw'\n",
    "\n",
    "#     os.remove(path + 'train.vw')\n",
    "#     os.remove(path + 'valid.vw')\n",
    "#     os.remove(path)\n",
    "\n",
    "    return train_score, valid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vw -c -k -d ../../data/data.hw8/tmp/train.vw -f ../../data/data.hw8/tmp/model.vw --passes 3\n",
      "vw -i ../../data/data.hw8/tmp/model.vw -t -d ../../data/data.hw8/tmp/train.vw -p ../../data/data.hw8/tmp/train_p.txt\n",
      "vw -i ../../data/data.hw8/tmp/model.vw -t -d ../../data/data.hw8/tmp/valid.vw -p ../../data/data.hw8/tmp/valid_p.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.94703338059999986, 0.94920024266666658)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_vw = '../../data/data.hw8/habr_train.vw'\n",
    "\n",
    "train_length = 120000\n",
    "test_fraction = 1/4\n",
    "train_size = int(train_length * (1-test_fraction))\n",
    "\n",
    "train_params = '--passes 3'\n",
    "to_dir = 'tmp'\n",
    "scoring = mean_absolute_error\n",
    "\n",
    "# holdout_cv_vw(train_file_vw = train_file_vw,\n",
    "#               train_size = train_size,\n",
    "#               train_params = train_params,\n",
    "#               to_dir = to_dir,\n",
    "#               scoring = scoring,\n",
    "#               make_sets = True,\n",
    "#               mute = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_vw(test_file_vw, model):\n",
    "    \n",
    "    \"\"\" This function takes a file path to vowpawabbit model\n",
    "        and a path to test data file in vw format, returns array of averaged\n",
    "        predictions\n",
    "        \n",
    "\n",
    "    Args:\n",
    "        test_file_vw: path to test data in vw format\n",
    "        model:        path to vw model\n",
    "\n",
    "    Returns:\n",
    "        array of predictions, averaged over all models\n",
    "    \"\"\"\n",
    "    \n",
    "    preds=[]\n",
    "\n",
    "    path = '/'.join(model.split('/')[:-1]) + '/'\n",
    "\n",
    "    command = 'vw -i ' + model + ' -t -d ' + test_file_vw + ' -p ' + path + 'tmp.txt'\n",
    "    print command\n",
    "    command = command.split()\n",
    "    call(command)\n",
    "\n",
    "    with open(path+'tmp.txt') as pred_file:\n",
    "        pred = [float(label) for label in pred_file.readlines()]\n",
    "\n",
    "    os.remove(path+'tmp.txt')\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use for randomized holdout\n",
    "\n",
    "# def holdout_cv_vw(train_file_vw,\n",
    "#                   train_params='',\n",
    "#                   to_dir = 'cv',\n",
    "#                   scoring=mean_absolute_error,\n",
    "#                   test_size=0.33,\n",
    "#                   random_state=42,\n",
    "#                   shuffle=False):\n",
    "    \n",
    "#     \"\"\" This function performs holdout cross validation with wovpalwabbit.\n",
    "#         It takes a path to training data in vw format as an argument and\n",
    "#         creates train and validation sets, as well a as model and prediction files\n",
    "#         in subdirectory to_dir. In the end, the model is retrained on both\n",
    "#         train and test data and saved as file. Temporaryfiles are removed.   \n",
    "\n",
    "#     Args:\n",
    "#         train_file_vw: path to training data in vw format\n",
    "#         train_params:  string of options for vowpal training \n",
    "#         to_dir:        name of the folder to save models to\n",
    "#         scoring:       pass function that returns score list of predictions and list of targets\n",
    "#         test_size:     size of holdout, same as rest_size for sklearn.train_test_split\n",
    "#         random_state:\n",
    "#         shuffle:\n",
    "\n",
    "#     Returns:\n",
    "#         a score on holdout set\n",
    "#         a path to the model\n",
    "#     \"\"\"\n",
    "    \n",
    "#     path = '/'.join(train_file_vw.split('/')[:-1]) + '/' + to_dir + '/'\n",
    "#     if not os.path.exists(path):\n",
    "#         os.makedirs(path)\n",
    "    \n",
    "#     # need codecs.open for texts with Cyrillic characters\n",
    "#     # without them, use simple open and drop 'utf-8'\n",
    "#     with codecs.open(train_file_vw, 'r', 'utf-8') as f:\n",
    "#         data = f.readlines()\n",
    "#     y = [float(s.split(' ', 1)[0]) for s in data]\n",
    "\n",
    "#     train, valid, _, valid_y = train_test_split(data, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "#     scores = []\n",
    "#     models = []\n",
    "\n",
    "#     with codecs.open(path+'train.vw', 'w', 'utf-8') as vw_train:\n",
    "#         for line in (train):\n",
    "#             vw_train.write(line)\n",
    "#     with codecs.open(path+'valid.vw', 'w', 'utf-8') as vw_train:\n",
    "#         for line in (valid):\n",
    "#             vw_train.write(line)\n",
    "\n",
    "#     #train a model\n",
    "#     command = 'vw -d ' + path+'train.vw -f ' + path+'model.vw ' + train_params\n",
    "#     print command\n",
    "#     command = command.split()\n",
    "#     call(command)\n",
    "  \n",
    "#     # make a prediction\n",
    "#     command = 'vw -i ' + path+'model.vw -t -d ' + \\\n",
    "#                path+'valid.vw -p ' + path+'pred.txt'\n",
    "#     print command\n",
    "#     command = command.split()\n",
    "#     call(command)\n",
    "\n",
    "#     with open(path+'pred.txt') as pred_file:\n",
    "#         pred = [float(label) for label in pred_file.readlines()]\n",
    "            \n",
    "#     scores.append(scoring(valid_y, pred))\n",
    "#     models.append(path+'model.vw')\n",
    "        \n",
    "# #         os.remove(path+'model'+str(k)+'.vw')\n",
    "#     os.remove(path+'pred.txt')\n",
    "\n",
    "#     os.remove(path + 'train.vw')\n",
    "#     os.remove(path + 'valid.vw')\n",
    "#     del data\n",
    "\n",
    "#     return scores, models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def kfold_cv_vw(train_file_vw,\n",
    "                train_params='',\n",
    "                to_dir = 'cv',\n",
    "                scoring=mean_absolute_error,\n",
    "                folds=5,\n",
    "                random_state=42,\n",
    "                shuffle=False):\n",
    "    \n",
    "    \"\"\" This function performs kfold cross validation with wovpalwabbit.\n",
    "        It takes a path to training data in vw format as an argument and\n",
    "        creates train and validation sets, as well as k model files\n",
    "        and k prediction files in to_dir subdirectory. Only model files are kept,\n",
    "        the rest are deleted in the end.        \n",
    "\n",
    "    Args:\n",
    "        train_file_vw: path to training data in vw format\n",
    "        train_params:  string of options for vowpal training \n",
    "        to_dir:        name of the folder to save models to\n",
    "        scoring:       pass function that returns score list of predictions and list of targets\n",
    "        folds:         number of folds, default is 5\n",
    "        random_state:\n",
    "        shuffle:\n",
    "\n",
    "    Returns:\n",
    "        list of k scores on k folds\n",
    "        list of paths to k models\n",
    "    \"\"\"\n",
    "    \n",
    "    path = '/'.join(train_file_vw.split('/')[:-1]) + '/' + to_dir + '/'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    # need codecs.open for texts with Cyrillic characters\n",
    "    # without them, use simple open and drop 'utf-8'\n",
    "    with codecs.open(train_file_vw, 'r', 'utf-8') as f:\n",
    "        data = f.readlines()\n",
    "    y = [float(s.split(' ', 1)[0]) for s in data]\n",
    "\n",
    "    kf = KFold(n_splits=folds, random_state=random_state, shuffle=shuffle)\n",
    "    scores = []\n",
    "    models = []\n",
    "\n",
    "    for k, split in enumerate(tqdm_notebook(kf.split(data), total=folds)):\n",
    "\n",
    "        # create train and validation sets for a given split\n",
    "        train = [data[i] for i in split[0]]\n",
    "        valid = [data[i] for i in split[1]]\n",
    "\n",
    "        valid_y = [y[i] for i in split[1]]\n",
    "\n",
    "        with codecs.open(path+'train.vw', 'w', 'utf-8') as vw_train:\n",
    "            for line in (train):\n",
    "                vw_train.write(line)\n",
    "        with codecs.open(path+'valid.vw', 'w', 'utf-8') as vw_train:\n",
    "            for line in (valid):\n",
    "                vw_train.write(line)\n",
    "\n",
    "        #train a model\n",
    "        command = 'vw -d ' + path+'train.vw -f ' + path+'model'+str(k)+'.vw ' + train_params\n",
    "        command = command.split()\n",
    "        call(command)\n",
    "  \n",
    "        # make a prediction\n",
    "        command = 'vw -i ' + path+'model'+str(k)+'.vw -t -d ' + \\\n",
    "                   path+'valid.vw -p ' + path+'pred'+str(k)+'.txt'\n",
    "        command = command.split()\n",
    "        call(command)\n",
    "\n",
    "        with open(path+'pred'+str(k)+'.txt') as pred_file:\n",
    "            pred = [float(label) for label in pred_file.readlines()]\n",
    "            \n",
    "        scores.append(scoring(valid_y, pred))\n",
    "        models.append(path+'model'+str(k)+'.vw')\n",
    "        \n",
    "#         os.remove(path+'model'+str(k)+'.vw')\n",
    "        os.remove(path+'pred'+str(k)+'.txt')\n",
    "\n",
    "    os.remove(path + 'train.vw')\n",
    "    os.remove(path + 'valid.vw')\n",
    "    del data\n",
    "\n",
    "    return scores, models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pred_ens_vw(test_file_vw, models_paths):\n",
    "    \n",
    "    \"\"\" This function takes a list of file paths to vowpawabbit models\n",
    "        and a path to test data file in vw format, returns array of averaged\n",
    "        predictions\n",
    "        \n",
    "\n",
    "    Args:\n",
    "        test_file_vw: path to test data in vw format\n",
    "        model_paths:  list of paths to vw models \n",
    "\n",
    "    Returns:\n",
    "        array of predictions, averaged over all models\n",
    "    \"\"\"\n",
    "    \n",
    "    preds=[]\n",
    "\n",
    "    for model in tqdm_notebook(models_paths, total=len(models_paths)):\n",
    "        path = '/'.join(model.split('/')[:-1]) + '/'\n",
    "\n",
    "        command = 'vw -i ' + model + ' -t -d ' + test_file_vw + ' -p ' + path+'tmp.txt'\n",
    "        command = command.split()\n",
    "        call(command)\n",
    "\n",
    "        with open(path+'tmp.txt') as pred_file:\n",
    "            pred = [float(label) for label in pred_file.readlines()]\n",
    "        preds.append(pred) \n",
    "        \n",
    "        os.remove(path+'tmp.txt')\n",
    "\n",
    "    return np.array(preds).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores, models = kfold_cv_vw(train_file_vw, train_params=train_params,\n",
    "#                              scoring=scoring, folds=folds,\n",
    "#                              random_state=random_state, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with codecs.open('../../data/data.hw8/cv/valid.vw', 'r', 'utf-8') as f:\n",
    "#     data = f.readlines()\n",
    "# valid_y = [float(s.split(' ', 1)[0]) for s in data]\n",
    "# del data\n",
    "\n",
    "# command = 'vw -d ../../data/data.hw8/cv/train.vw -f ../../data/data.hw8/cv/model.vw' + \\\n",
    "# ' --passes 3 --cache_file ../../data/data.hw8/cv/train.cache'\n",
    "# call(command.split())\n",
    "# !vw -d ../../data/data.hw8/cv/train.vw -f ../../data/data.hw8/cv/model.vw \\\n",
    "# --passes 3 --cache_file ../../data/data.hw8/cv/train.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# command = 'vw -i ../../data/data.hw8/cv/model.vw -t -d ../../data/data.hw8/cv/valid.vw ' + \\\n",
    "#           '-p ../../data/data.hw8/cv/pred.txt'\n",
    "# call(command.split())\n",
    "\n",
    "# with open('../../data/data.hw8/cv/pred.txt') as pred_file:\n",
    "#     pred = [float(label) for label in pred_file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_absolute_error(pred, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check stdout here\n",
    "\n",
    "# path = '../../data/data.hw8/cv/'\n",
    "# command = 'vw -d ../../data/data.hw8/cv/train.vw -f ../../data/data.hw8/cv/model.vw'\n",
    "# p = Popen(command.split(), stdout=PIPE)\n",
    "# print p.communicate()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
